# -*- coding: utf-8 -*-
"""MLOps_project_model1_v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v37hT6KpcYNCTMuP3adJzmfRici8AxKS
"""
import cv2
import io
from PIL import Image
import os, time
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms.functional as F
import shutil
import random
import numpy as np

def read_label(label_file: str):
    with open(label_file, "rb") as f:
        return np.frombuffer(f.read(), np.uint32, -1).astype("int32")

class AdobeVFRDataset(Dataset):
    def __init__(self, bcf_dir: str, split_name: str, transform=None):
        self.bcf_filename = os.path.join(bcf_dir, f"{split_name}.bcf")
        self.labels = read_label(os.path.join(bcf_dir, f"{split_name}.label"))
        self.num_labels = len(np.unique(self.labels))
        self.transform = transform
        self._load_bcf_meta()

    def __len__(self):
        return len(self._bcf_offsets) - 1

    def __getitem__(self, index):
        binary_image = self._get_bcf_entry_by_index(index)
        image_array = np.asarray(bytearray(io.BytesIO(binary_image).read()), dtype=np.uint8)
        opencv_image = cv2.imdecode(np.frombuffer(image_array, np.uint8), -1)
        pil_image = Image.fromarray(opencv_image)
        x = self.transform(pil_image) if self.transform else pil_image
        y = torch.as_tensor(self.labels[index], dtype=torch.long)
        return x, y

    def _load_bcf_meta(self):
        with open(self.bcf_filename, "rb") as f:
            size = np.frombuffer(f.read(8), dtype=np.uint64)[0]
            file_sizes = np.frombuffer(f.read(int(8 * size)), dtype=np.uint64)
            self._bcf_offsets = np.append(np.uint64(0), np.add.accumulate(file_sizes))

    def _get_bcf_entry_by_index(self, i):
        with open(self.bcf_filename, "rb") as f:
            f.seek((len(self._bcf_offsets) * 8 + self._bcf_offsets[i]).astype("uint64"))
            return f.read(self._bcf_offsets[i + 1] - self._bcf_offsets[i])
            
### Configuration
config = {
    "initial_epochs": 5,
    "total_epochs": 20,
    "patience": 5,
    "batch_size": 32,
    "lr": 1e-4,
    "fine_tune_lr": 1e-5,
    "dropout_probability": 0.5,
    "random_horizontal_flip": 0.5,
    "random_rotation": 15,
    "color_jitter_brightness": 0.2,
    "color_jitter_contrast": 0.2,
    "color_jitter_saturation": 0.2,
    "color_jitter_hue": 0.1,
    "num_classes": 200,
    "data_dir": os.getenv("FONTS_DATA_DIR", "/mnt/block20/adobe_vfr")
}

### Transforms
train_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(p=config["random_horizontal_flip"]),
    transforms.RandomRotation(config["random_rotation"]),
    transforms.ColorJitter(
        brightness=config["color_jitter_brightness"],
        contrast=config["color_jitter_contrast"],
        saturation=config["color_jitter_saturation"],
        hue=config["color_jitter_hue"]
    ),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

### Datasets & Loaders
data_dir = os.getenv("FONTS_DATA_DIR", "/mnt/block20/adobevfr")
train_dir = os.path.join(data_dir, "training")
val_dir = os.path.join(data_dir, "validation")
test_dir = os.path.join(data_dir, "evaluation")

train_dataset = AdobeVFRDataset(train_dir, "train", transform=train_transform)
val_dataset = AdobeVFRDataset(val_dir, "val", transform=val_test_transform)
test_dataset = AdobeVFRDataset(test_dir, "test", transform=val_test_transform)

save_path = os.getenv("MODEL_SAVE_PATH", "/mnt/block20/best_resnet50.pth")

train_loader = DataLoader(train_dataset, batch_size=config["batch_size"], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config["batch_size"], shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=config["batch_size"], shuffle=False)
config["num_classes"] = train_dataset.num_labels

### Model
resnet50_model = models.resnet50(weights='IMAGENET1K_V1')  # or weights=None if training from scratch
num_ftrs = resnet50_model.fc.in_features
resnet50_model.fc = nn.Sequential(
    nn.Dropout(config["dropout_probability"]),
    nn.Linear(num_ftrs, config["num_classes"])
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet50_model = resnet50_model.to(device)

### Freeze Backbone Initially
for param in resnet50_model.parameters():
    param.requires_grad = False
for param in resnet50_model.fc.parameters():
    param.requires_grad = True

### Optimizer and Loss
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(resnet50_model.fc.parameters(), lr=config["lr"])

### Training Function
def train(model, loader, criterion, optimizer, device):
    model.train()
    total_loss, correct, total = 0.0, 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
    return total_loss / len(loader), correct / total

### Validation Function
def validate(model, loader, criterion, device):
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return total_loss / len(loader), correct / total

### Phase 1: Train Head Only
best_val_loss = float('inf')
for epoch in range(config["initial_epochs"]):
    start_time = time.time()
    train_loss, train_acc = train(resnet50_model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = validate(resnet50_model, val_loader, criterion, device)
    print(f"[Init Epoch {epoch+1}] Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Time: {time.time() - start_time:.2f}s")
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(resnet50_model.state_dict(), save_path)

        print("  ↳ Model saved.")

### Phase 2: Unfreeze and Fine-tune All
for param in resnet50_model.parameters():
    param.requires_grad = True

optimizer = optim.Adam(resnet50_model.parameters(), lr=config["fine_tune_lr"])
patience_counter = 0

for epoch in range(config["initial_epochs"], config["total_epochs"]):
    start_time = time.time()
    train_loss, train_acc = train(resnet50_model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = validate(resnet50_model, val_loader, criterion, device)
    print(f"[Finetune Epoch {epoch+1}] Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Time: {time.time() - start_time:.2f}s")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(resnet50_model.state_dict(), save_path)

        patience_counter = 0
        print("  ↳ Model saved.")
    else:
        patience_counter += 1
        print(f"  ↳ No improvement. Patience: {patience_counter}")
        if patience_counter >= config["patience"]:
            print("  ↳ Early stopping.")
            break

### Final Evaluation
resnet50_model.load_state_dict(torch.load(save_path))
test_loss, test_acc = validate(resnet50_model, test_loader, criterion, device)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

