{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXG9BSLgSTUm"
      },
      "outputs": [],
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/mnt/font-detector/models/\")"
      ],
      "metadata": {
        "id": "yAih3RXT_Mi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFontAutoencoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 64, 12, 2, 1), torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.BatchNorm2d(64), torch.nn.MaxPool2d(2, 2),\n",
        "            torch.nn.Conv2d(64, 128, 3, 1, 1), torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.BatchNorm2d(128), torch.nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "class DeepFont(torch.nn.Module):\n",
        "    def __init__(self, ae_encoder: torch.nn.Sequential, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.ae_encoder = ae_encoder\n",
        "        self.conv5 = torch.nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.conv6 = torch.nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.conv7 = torch.nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(256 * 12 * 12, 4096)\n",
        "        self.drop1 = torch.nn.Dropout(0.5)\n",
        "        self.fc2 = torch.nn.Linear(4096, 4096)\n",
        "        self.drop2 = torch.nn.Dropout(0.5)\n",
        "        self.fc3 = torch.nn.Linear(4096, 100)  # assuming 100 fonts\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ae_encoder(x)\n",
        "        x = torch.relu(self.conv5(x))\n",
        "        x = torch.relu(self.conv6(x))\n",
        "        x = torch.relu(self.conv7(x))\n",
        "        x = self.flatten(x)\n",
        "        x = self.drop1(torch.relu(self.fc1(x)))\n",
        "        x = self.drop2(torch.relu(self.fc2(x)))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "WSa0R_jH_OiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────── deterministic preprocessing ─────────────────\n",
        "class ResizeHeight:\n",
        "    \"\"\"Resize so the height equals `height`, keep aspect ratio.\"\"\"\n",
        "    def __init__(self, height: int = 105):\n",
        "        self.height = height\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        w, h = img.size\n",
        "        if h == 0:\n",
        "            return Image.new(img.mode, (1, self.height), 255)\n",
        "        new_w = max(1, round(self.height * w / h))\n",
        "        return img.resize((new_w, self.height), Image.LANCZOS)\n",
        "\n",
        "\n",
        "class Squeezing:\n",
        "    \"\"\"Deterministically squeeze horizontally by a fixed factor.\"\"\"\n",
        "    def __init__(self, ratio: float = 2.5):\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        _, h = img.size\n",
        "        return img.resize((max(1, round(h * self.ratio)), h),\n",
        "                          Image.LANCZOS)\n",
        "\n",
        "class CenterPatch:\n",
        "    \"\"\"Extract (or pad) a centred width-`step` patch.\"\"\"\n",
        "    def __init__(self, step: int = 105):\n",
        "        self.step = step\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad(img: Image.Image, width: int) -> Image.Image:\n",
        "        w, h = img.size\n",
        "        if w >= width:\n",
        "            return img\n",
        "        canvas = Image.new(img.mode, (width, h), 255)\n",
        "        canvas.paste(img, (0, 0))\n",
        "        return canvas\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        img = self._pad(img, self.step)\n",
        "        w, h = img.size\n",
        "        if w == self.step:\n",
        "            return img\n",
        "        sx = (w - self.step) // 2\n",
        "        tile = img.crop((sx, 0, sx + self.step, h))\n",
        "        return tile\n"
      ],
      "metadata": {
        "id": "gicCEYUK_R8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "model_path = \"/mnt/font-detector/models/finetuned_n100.pt\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ae = DeepFontAutoencoder()\n",
        "model = DeepFont(ae.encoder, num_classes=100)  # or 25 if you're using the canary model\n",
        "\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Missing keys:\", missing)\n",
        "print(\"Unexpected keys:\", unexpected)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "5f2tw1LLSiur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "font_data_dir = os.getenv(\"FONT_DATA_DIR\", \"/mnt/evaluation_filtered\")\n",
        "subset_font_path = \"/home/jovyan/work/fontsubset.txt\"\n",
        "\n",
        "# Load font name to index map\n",
        "with open(subset_font_path) as f:\n",
        "    font_list = [line.strip() for line in f if line.strip()]\n",
        "font_to_index = {name: idx for idx, name in enumerate(font_list)}\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    ResizeHeight(105),\n",
        "    Squeezing(),\n",
        "    CenterPatch(step=105),\n",
        "    transforms.Grayscale(1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class FontDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform, font_to_index):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.font_to_index = font_to_index\n",
        "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith(\".png\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.image_files[idx]\n",
        "        txt_filename = img_filename.replace(\".png\", \".txt\")\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        txt_path = os.path.join(self.img_dir, txt_filename)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        with open(txt_path) as f:\n",
        "            font_name = f.read().strip()\n",
        "\n",
        "        label = self.font_to_index[font_name]\n",
        "        return image, label\n",
        "\n",
        "test_dataset = FontDataset(font_data_dir, val_test_transform, font_to_index)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kZWPOHKJSnc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure model is on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()\n",
        "\n",
        "# Preallocate arrays\n",
        "dataset_size = len(test_loader.dataset)\n",
        "all_predictions = np.empty(dataset_size, dtype=np.int64)\n",
        "all_labels = np.empty(dataset_size, dtype=np.int64)\n",
        "\n",
        "# Evaluation loop\n",
        "current_index = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        batch_size = labels.size(0)\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_predictions[current_index:current_index + batch_size] = predicted.cpu().numpy()\n",
        "        all_labels[current_index:current_index + batch_size] = labels.cpu().numpy()\n",
        "        current_index += batch_size\n"
      ],
      "metadata": {
        "id": "teUFsJkKTmAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "overall_accuracy = (all_predictions == all_labels).sum() / all_labels.shape[0] * 100\n",
        "print(f'Overall Accuracy: {overall_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "lYLOcG8ITt3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "with open(\"/home/jovyan/work/fontsubset.txt\", \"r\") as f:\n",
        "    classes = np.array([line.strip() for line in f if line.strip()])\n",
        "\n",
        "num_classes = classes.shape[0]\n"
      ],
      "metadata": {
        "id": "dNFVLoVYT35B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "per_class_correct = np.zeros(num_classes, dtype=np.int32)\n",
        "per_class_total = np.zeros(num_classes, dtype=np.int32)\n",
        "\n",
        "for true_label, pred_label in zip(all_labels, all_predictions):\n",
        "    per_class_total[true_label] += 1\n",
        "    per_class_correct[true_label] += int(true_label == pred_label)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    if per_class_total[i] > 0:\n",
        "        acc = per_class_correct[i] / per_class_total[i] * 100\n",
        "        correct_str = f\"{per_class_correct[i]}/{per_class_total[i]}\"\n",
        "        print(f\"{classes[i]:<20} {acc:10.2f}% {correct_str:>20}\")"
      ],
      "metadata": {
        "id": "28wDM-ZpT-fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "# Initialize full confusion matrix\n",
        "conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
        "\n",
        "# Fill it using your predictions\n",
        "for true_label, pred_label in zip(all_labels, all_predictions):\n",
        "    conf_matrix[true_label, pred_label] += 1\n",
        "\n",
        "# Select first 25 class indices (change this as needed)\n",
        "subset_indices = list(range(25))\n",
        "subset_matrix = conf_matrix[np.ix_(subset_indices, subset_indices)]\n",
        "subset_class_names = [classes[i] for i in subset_indices]\n",
        "\n",
        "# Plot the 5x5 heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(subset_matrix, annot=True, fmt='d',\n",
        "            xticklabels=subset_class_names,\n",
        "            yticklabels=subset_class_names,\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (First 25 Classes)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c2OohvasUAbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "font_1 = \"CourierStd\"\n",
        "font_2 = \"AmigoStd\"\n",
        "\n",
        "font1_index = np.where(classes == font_1)[0][0]\n",
        "font2_index = np.where(classes == font_2)[0][0]\n",
        "\n",
        "confused_indices = [i for i, (t, p) in enumerate(zip(all_labels, all_predictions))\n",
        "                    if (t == font1_index and p == font2_index) or (t == font2_index and p == font1_index)]\n",
        "\n",
        "sample_indices = np.random.choice(confused_indices, size=min(5, len(confused_indices)), replace=False)\n",
        "\n",
        "# For controlled demo/discussion, override with hardcoded indices (optional)\n",
        "# sample_indices = np.array([404, 927, 496, 435, 667])  # ← only use this if you know these exist in your data\n"
      ],
      "metadata": {
        "id": "F1TqEZ97UP1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "sample_images = []\n",
        "global_index = 0\n",
        "\n",
        "for images, _ in test_loader:\n",
        "    batch_size = images.size(0)\n",
        "    for idx in sample_indices:\n",
        "        if global_index <= idx < global_index + batch_size:\n",
        "            image = images[idx - global_index].cpu()\n",
        "            sample_images.append((idx, image))\n",
        "    global_index += batch_size\n",
        "    if len(sample_images) == len(sample_indices):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "ArmXvhoBUUR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "# Visualize those samples (undo the normalization first)\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i, (idx, image) in enumerate(sample_images):\n",
        "    image = image * std[:, None, None] + mean[:, None, None]  # unnormalize\n",
        "    image = torch.clamp(image, 0, 1)\n",
        "    image = image.permute(1, 2, 0)  # go from \"channels, height, width\" format to \"height, width, channels\"\n",
        "    plt.subplot(1, len(sample_images), i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"True: {classes[all_labels[idx]]}\\nPred: {classes[all_predictions[idx]]}\\nIndex: {idx}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RpVqcB1MUUJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# GradCAM setup\n",
        "target_layer = model.features[-1]\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])"
      ],
      "metadata": {
        "id": "Sq0zHIpnU795"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i, (idx, image) in enumerate(sample_images):\n",
        "    input_tensor = (image.clone() - mean[:, None, None]) / std[:, None, None]  # normalize\n",
        "    input_tensor = input_tensor.unsqueeze(0)  # add batch dim\n",
        "\n",
        "    target_category = int(all_predictions[idx])\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(target_category)])\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    image_disp = image * std[:, None, None] + mean[:, None, None]  # unnormalize\n",
        "    image_disp = torch.clamp(image_disp, 0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "    visualization = show_cam_on_image(image_disp, grayscale_cam, use_rgb=True)\n",
        "    plt.subplot(1, len(sample_images), i + 1)\n",
        "    plt.imshow(visualization)\n",
        "    plt.title(f\"True: {classes[all_labels[idx]]}\\nPred: {classes[all_predictions[idx]]}\\nIndex: {idx}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-wk7kWdEU7z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "template based tests"
      ],
      "metadata": {
        "id": "RVTLU8YuVjxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "TEMPLATE_DIR = \"templates\"\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(8, 5))\n",
        "\n",
        "# Fonts row\n",
        "font_dir = os.path.join(TEMPLATE_DIR, \"fonts\")\n",
        "font_classes = [d for d in os.listdir(font_dir) if os.listdir(os.path.join(font_dir, d))]\n",
        "random_font = random.choice(font_classes)\n",
        "font_images = random.sample(os.listdir(os.path.join(font_dir, random_font)), 3)\n",
        "font_paths = [os.path.join(font_dir, random_font, f) for f in font_images]\n",
        "\n",
        "for i, path in enumerate(font_paths):\n",
        "    axes[0, i].imshow(Image.open(path))\n",
        "    axes[0, i].set_title(f\"Font ({random_font})\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "# Backgrounds row\n",
        "bg_dir = os.path.join(TEMPLATE_DIR, \"background\")\n",
        "bg_images = random.sample(os.listdir(bg_dir), 3)\n",
        "bg_paths = [os.path.join(bg_dir, f) for f in bg_images]\n",
        "\n",
        "for i, path in enumerate(bg_paths):\n",
        "    axes[1, i].imshow(Image.open(path))\n",
        "    axes[1, i].set_title(\"Background\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eEfdBxd8V4Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compose_font_image(font_path, bg_path=None, extra_path=None):\n",
        "    \"\"\"\n",
        "    Composes a font image with optional background and optional overlay.\n",
        "    All inputs are image file paths.\n",
        "    Returns a final composited RGB image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the font image and convert to RGBA\n",
        "    font_img = Image.open(font_path).convert(\"RGBA\")\n",
        "\n",
        "    # Background setup\n",
        "    if bg_path:\n",
        "        bg = Image.open(bg_path).convert(\"RGBA\").resize(font_img.size)\n",
        "    else:\n",
        "        bg = Image.new(\"RGBA\", font_img.size, (255, 255, 255, 255))\n",
        "\n",
        "    bg_w, bg_h = bg.size\n",
        "    y_offset = int(bg_h * 0.05)\n",
        "    # Resize font image (scale down slightly)\n",
        "    font_scale = 0.8\n",
        "    font_img = font_img.resize((int(bg_w * font_scale), int(bg_h * font_scale)))\n",
        "    ft_w, ft_h = font_img.size\n",
        "\n",
        "    # Paste font image last (so it appears above the extras)\n",
        "    bg.paste(font_img, ((bg_w - ft_w) // 2, bg_h - ft_h - y_offset), font_img)\n",
        "    # Optional overlay (e.g., emoji, occlusion, sticker)\n",
        "    if extra_path:\n",
        "        extra_scale = 0.35\n",
        "        extra = Image.open(extra_path).convert(\"RGBA\")\n",
        "        extra = extra.resize((int(bg_w * extra_scale), int(bg_h * extra_scale)))\n",
        "        ex_w, ex_h = extra.size\n",
        "        bg.paste(extra, (bg_w - ex_w, bg_h - ex_h - y_offset), extra)\n",
        "\n",
        "\n",
        "\n",
        "    return bg.convert(\"RGB\")\n"
      ],
      "metadata": {
        "id": "xnXyQom5WDit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CourierStd"
      ],
      "metadata": {
        "id": "SRmrMWXymG60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "imgs = {\n",
        "    'original_image': compose_font_image('templates/fonts/CourierStd/CourierStd_0.png'),\n",
        "    'composed_bg1_extra1': compose_font_image('templates/fonts/CourierStd/CourierStd_0.png',\n",
        "                                              'templates/background/001.png',\n",
        "                                              'templates/extras/smiley_face.png'),\n",
        "    'composed_bg2_extra2': compose_font_image('templates/fonts/CourierStd/CourierStd_0.png',\n",
        "                                              'templates/background/002.png',\n",
        "                                              'templates/extras/circle.png'),\n",
        "    'composed_same_class': compose_font_image('templates/fonts/CourierStd/CourierStd_1.png',\n",
        "                                              'templates/background/001.png'),\n",
        "    'composed_diff_class': compose_font_image('templates/fonts/ImpactLTStd/ImpactLTStd_2.png',\n",
        "                                              'templates/background/001.png')\n",
        "}\n"
      ],
      "metadata": {
        "id": "0J8sZV-tWNPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(14, 3))\n",
        "\n",
        "for ax, key in zip(axes, imgs.keys()):\n",
        "    ax.imshow(imgs[key].resize((105, 105)).crop((16, 16, 224, 224)))\n",
        "    ax.set_title(f\"{key}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UD4lXJ1mWOxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, image, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "    model.eval()\n",
        "    image_tensor = val_test_transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        return output.argmax(dim=1).item()\n"
      ],
      "metadata": {
        "id": "DxpVaJDTWbGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "\n",
        "for i, key in enumerate(imgs.keys()):\n",
        "    image_np = np.array(imgs[key].resize((105, 105))).astype(dtype=np.float32) / 255.0\n",
        "    pred = predict(model, imgs[key])\n",
        "\n",
        "    input_tensor = val_test_transform(imgs[key]).unsqueeze(0)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
        "    vis = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    axes[0, i].imshow(imgs[key].resize((224, 224)))\n",
        "    axes[0, i].set_title(f\"{key}\\nPredicted: {pred} ({classes[pred]})\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    axes[1, i].imshow(vis)\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHaFtBKKWeHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AmigoStd"
      ],
      "metadata": {
        "id": "a3ElFHPImCEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = {\n",
        "    'original_image': compose_font_image('templates/fonts/AmigoStd/AmigoStd_0.png'),\n",
        "    'composed_bg1_extra1': compose_font_image('templates/fonts/AmigoStd/AmigoStd_0.png',\n",
        "                                              'templates/background/003.png',\n",
        "                                              'templates/extras/smiley_face.png'),\n",
        "    'composed_bg2_extra2': compose_font_image('templates/fonts/AmigoStd/AmigoStd_0.png',\n",
        "                                              'templates/background/002.png',\n",
        "                                              'templates/extras/arrow.png'),\n",
        "    'composed_same_class': compose_font_image('templates/fonts/AmigoStd/AmigoStd_1.png',\n",
        "                                              'templates/background/003.png'),\n",
        "    'composed_diff_class': compose_font_image('templates/fonts/CourierStd/CourierStd_2.png',\n",
        "                                              'templates/background/003.png')\n",
        "}\n"
      ],
      "metadata": {
        "id": "almYESn4Wd4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "\n",
        "for i, key in enumerate(imgs.keys()):\n",
        "    image_np = np.array(imgs[key].resize((105, 105))).astype(dtype=np.float32) / 255.0\n",
        "    pred = predict(model, imgs[key])\n",
        "\n",
        "    input_tensor = val_test_transform(imgs[key]).unsqueeze(0)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
        "    vis = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    axes[0, i].imshow(imgs[key].resize((224, 224)))\n",
        "    axes[0, i].set_title(f\"{key}\\nPredicted: {pred} ({classes[pred]})\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    axes[1, i].imshow(vis)\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t5lk0EgBWnE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impact"
      ],
      "metadata": {
        "id": "kyJ0Z1Mflxk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = {\n",
        "    'original_image': compose_font_image('templates/fonts/ImpactLTStd/ImpactLTStd_0.png'),\n",
        "    'composed_bg1_extra1': compose_font_image('templates/fonts/ImpactLTStd/ImpactLTStd_0.png',\n",
        "                                              'templates/background/003.png',\n",
        "                                              'templates/extras/smiley_face.png'),\n",
        "    'composed_bg2_extra2': compose_font_image('templates/fonts/ImpactLTStd/ImpactLTStd_0.png',\n",
        "                                              'templates/background/002.png',\n",
        "                                              'templates/extras/arrow.png'),\n",
        "    'composed_same_class': compose_font_image('templates/fonts/ImpactLTStd/ImpactLTStd_1.png',\n",
        "                                              'templates/background/003.png'),\n",
        "    'composed_diff_class': compose_font_image('templates/fonts/AmigoStd/AmigoStd_2.png',\n",
        "                                              'templates/background/003.png')\n",
        "}\n"
      ],
      "metadata": {
        "id": "JVo6rlkZlwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "\n",
        "for i, key in enumerate(imgs.keys()):\n",
        "    image_np = np.array(imgs[key].resize((105, 105))).astype(dtype=np.float32) / 255.0\n",
        "    pred = predict(model, imgs[key])\n",
        "\n",
        "    input_tensor = val_test_transform(imgs[key]).unsqueeze(0)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
        "    vis = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    axes[0, i].imshow(imgs[key].resize((224, 224)))\n",
        "    axes[0, i].set_title(f\"{key}\\nPredicted: {pred} ({classes[pred]})\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    axes[1, i].imshow(vis)\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXnkdA7alwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like test"
      ],
      "metadata": {
        "id": "9tseRfQKWo87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# runs in jupyter container on node-eval-offline\n",
        "\n",
        "gibberish_dir = \"gibberish_looks_like\"\n",
        "font_folders = [f for f in os.listdir(gibberish_dir) if os.path.isdir(os.path.join(gibberish_dir, f))]\n",
        "\n",
        "selected_images = []\n",
        "\n",
        "# Sample one gibberish image from each font\n",
        "for font_folder in font_folders:\n",
        "    folder_path = os.path.join(gibberish_dir, font_folder)\n",
        "    images = [f for f in os.listdir(folder_path) if f.endswith(\".png\")]\n",
        "    if images:\n",
        "        chosen = random.choice(images)\n",
        "        selected_images.append((font_folder, os.path.join(folder_path, chosen)))\n",
        "\n",
        "# Plot the selected gibberish samples\n",
        "fig, axes = plt.subplots(1, len(selected_images), figsize=(5 * len(selected_images), 3))\n",
        "\n",
        "for ax, (font_name, img_path) in zip(axes, selected_images):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    pred = predict(model, image)\n",
        "\n",
        "    ax.imshow(image.resize((224, 224)).crop((16, 16, 224, 224)))\n",
        "    ax.set_title(f\"{font_name}\\nPred: {classes[pred]}\", fontsize=8)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pgiYrlhZWocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Suite"
      ],
      "metadata": {
        "id": "gWgDD8lIYiYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --verbose --tb=no tests/"
      ],
      "metadata": {
        "id": "v3kI16AXYhZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests/test_fonts.py\n",
        "def test_prediction_format():\n",
        "    pred = model.predict(image)\n",
        "    assert isinstance(pred, int)\n"
      ],
      "metadata": {
        "id": "HOcDm-tpYl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --verbose --lf --tb=no tests/\n"
      ],
      "metadata": {
        "id": "-MHAOQJHYn2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --verbose --tb=no tests/test_fontdetector_test_cases.py\n"
      ],
      "metadata": {
        "id": "MaX__oHUYo-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}