# ML Ops Project
This is the ML Ops project, a deep-learning-based tool that detects which font from a database of fonts given an image of some characters. 


## Font Detector
**FontDetector** is a machine learning system that integrates into existing graphic design or branding workflows to automatically detect fonts from uploaded images. In current industry practice, font matching is done manually or with inconsistent third-party tools. FontDetector improves accuracy and reduces time spent on font identification by using deep learning and image processing. The business value lies in improving designer productivity and accuracy, especially in marketing, publishing, and type design work.

Business Metrics:
* Font Identification Accuracy (top-1 and top-3 match)
* Reduction in time spent per font match
* Number of successful matches without manual intervention
  
<!-- 
Discuss: Value proposition: Your will propose a machine learning system that can be 
used in an existing business or service. (You should not propose a system in which 
a new business or service would be developed around the machine learning system.) 
Describe the value proposition for the machine learning system. What’s the (non-ML) 
status quo used in the business or service? What business metric are you going to be 
judged on? (Note that the “service” does not have to be for general users; you can 
propose a system for a science problem, for example.)
-->

### Contributors

<!-- Table of contributors and their roles. 
First row: define responsibilities that are shared by the team. 
Then, each row after that is: name of contributor, their role, and in the third column, 
you will link to their contributions. If your project involves multiple repos, you will 
link to their contributions in all repos here. -->

| Name                            | Responsible for (subject to change) | Link to their commits in this repo |
|---------------------------------|-----------------|------------------------------------|
| All team members                |Overall design, integration, and testing|                                    |
| Team member 1: Mukund Ramakrishnan           |Model training and training platforms (units 4-5)|        *To be added*                            |
| Team member 2: Alex Gonzalez   |Model serving and monitoring (units 6-7)|            *To be added*                        |
| Team member 3: Austin Ebel                    |Data pipeline (unit 8)|         *To be added*                           |


### System diagram
![FontDetector (2)](https://github.com/user-attachments/assets/d3127bb7-0cc6-4ba5-ad13-821793ebe43e)

### Summary of outside materials

<!-- In a table, a row for each dataset, foundation model. 
Name of data/model, conditions under which it was created (ideally with links/references), 
conditions under which it may be used. -->

|              | How it was created | Conditions of use |
|--------------|--------------------|-------------------|
| Data set 1: Adobe VFR Font Set   | Adobe created this dataset as part of their Visual Font Recognition (VFR) research. It includes labeled images of real-world font renderings collected from diverse sources and manually annotated for font classification                    |     Adobe VFR is publicly released for non-commercial, academic use only.              |
| Data set 2: Google Fonts (Rendered Synthetic Images)   | This dataset was synthetically generated by rendering sample text using fonts from the Google Fonts open-source library. Each font was applied to a predefined string using a Python script with PIL to generate consistent labeled training images                    |   All fonts in the Google Fonts library are licensed under open-source licenses. The generated images are free to use for commercial and academic purposes as long as the original font licenses are respected.                |
| Base model 1: ResNet18 |  ResNet18 is a convolutional neural network architecture pretrained on the ImageNet dataset. It was adapted to the font classification task through transfer learning by fine-tuning it on the font image datasets.                  | ResNet18 is available under an open-source license (MIT License) and can be used freely for both academic and commercial projects.                   |
| etc          |                    |                   |


### Summary of infrastructure requirements

<!-- Itemize all your anticipated requirements: What (`m1.medium` VM, `gpu_mi100`), 
how much/when, justification. Include compute, floating IPs, persistent storage. 
The table below shows an example, it is not a recommendation. -->

| Requirement     | How many/when                                     | Justification |
|-----------------|---------------------------------------------------|---------------|
| `m1.medium` VMs | 3 for entire project duration                     | For Backend services, model orchestration, and frontent           |
| `gpu_mi100`     | 4 hour block twice a week                         |  For model training and fine-tuning on image data             |
| Floating IPs    | 1 for entire project duration, 1 for sporadic use |      To host the frontend uploader + model inference API         |
| *Volume Storage*     (unsure)        |           50GB persistent Volume                                        |        To store datasets and model checkpoints       |

### Detailed design plan

<!-- In each section, you should describe (1) your strategy, (2) the relevant parts of the 
diagram, (3) justification for your strategy, (4) relate back to lecture material, 
(5) include specific numbers. -->

#### Model training and training platforms
**Strategy**: 
We will use transfer learning by fine-tuning a pretrained **ResNet18** model on our font classification datasets. The datasets include Adobe VFR and synthetic Google Fonts rendered via a Python pipeline. Training will include data augmentation to increase robustness.

**Diagram Components:**
* Datasets stored in Chameleon persistent volume
* ResNet18 fine-tuned in a GPU container (PyTorch)
* MLflow for experiment tracking
* Ray Train for distributed training
* Ray Tune for hyperparameter optimization 

**Justification:**
This approach balances performance and speed. ResNet18 provides good results with manageable training times. Training on both synthetic and real data improves generalization. Training on Chameleon using Ray allows scheduling and reproducibility.

**Lecture Tie-Ins:**
* Unit 4: Transfer Learning, scale with Ray
* Unit 5: Infrastructure provisioning and containerization
* Difficulty Point: Ray Train and Ray Tune integration for scalable and tuned training

**Numbers:**
* Training time per run: 45 minutes per model on GPU
* Dataset size: ~25,000 images
* Target: >= 90% top-1 accuracy
* Tuning parameters: learning rate, batch size, dropout, augmentation strength
  
<!-- Make sure to clarify how you will satisfy the Unit 4 and Unit 5 requirements, 
and which optional "difficulty" points you are attempting. -->

#### Model serving and monitoring platforms
**Strategy:** 
The model will be wrapped in a **FastAPI service** and served via an inference container. We will expose a REST endpoint (/predict) that accepts image uploads and returns the top-1 font predicitons. We will deploy two model versions and route traffic using basic logic to support canary evaluation.

**Diagram Components:**
* FastAPI container exposing predict and metrics
* Canary routing script for controlled deployment testing
  

**Justification:**
FastAPI is a fast and lightweight framework suitable for real-time inference. Canary evaluation allows us to test updated models in production with limited user exposure before full deployment.

**Lecture Tie-Ins:**
* Unit 6: Containerized API serving, latency analysis
* Unit 7: Monitoring with
* Difficulty Point: Canary Evaluation with live traffic splitting and monitoring

**Metrics:**
* Inference latency target: <300ms per request
* Throughput: 10-20 concurrent requests under load
* Serving from both CPU and GPU environments will be benchmarked

<!-- Make sure to clarify how you will satisfy the Unit 6 and Unit 7 requirements, 
and which optional "difficulty" points you are attempting. -->

#### Data pipeline

We will create an ETL pipeline (extract-transfer-load) and with that, we will implement data cleaning and labeling, and store that data in persistent volumes on Chameleon. We will write a script and implement tools to extract text from the "real world" (scans of fonts in the wild) to simulate production use. This simulated data will incorporate scanned images from a variety of sources, such as magazines like The New Yorker, printed books, advertising flyers, and similar texts. 


#### Continuous X

We will create a Continuous X pipeline to automate and streamline the entire machine learning lifecycle, ensuring consistency, reliability, and reproducibility of our system. This will include Continuous Integration (CI), Continuous Deployment (CD), and Continuous Training (CT).

For CI/CD, we will use ArgoCD to define automated workflows for building, testing, and deploying our model. When new code is pushed or a pull request is made, the CI pipeline will trigger unit tests, model validation, and container builds. Successful builds will then be deployed to staging environments for testing.

For Continuous Training, we will configure an automated re-training pipeline that triggers when new training data becomes available or when model performance degrades in production (detected via our monitoring tools). This pipeline will run on the Chameleon infrastructure using Ray clusters to perform distributed training efficiently. The retraining process will include hyperparameter tuning through Ray Tune to continuously improve model accuracy.



(Our two difficulty points are Ray Train and Canary Evaluation). 
